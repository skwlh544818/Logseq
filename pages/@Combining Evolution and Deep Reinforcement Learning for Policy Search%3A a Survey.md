tags:: [[/reading]], [[å¾…è¯»]], [[é‡è¦]]
date:: 2022
archive-location:: 5 ğŸ“Š
extra:: "Publisher: ACM New York, NY"
doi:: 10.1145/3569096
title:: @Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey
item-type:: [[journalArticle]]
original-title:: Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey
short-title:: Combining Evolution and Deep Reinforcement Learning for Policy Search
publication-title:: ACM Transactions on Evolutionary Learning
authors:: [[Olivier Sigaud]]
library-catalog:: Google Scholar
links:: [Local library](zotero://select/library/items/K85IPVTQ), [Web library](https://www.zotero.org/users/8746250/items/K85IPVTQ)

- [[Attachments]]
	- [Full Text](https://dl.acm.org/doi/pdf/10.1145/3569096) {{zotero-imported-file X5VJGPV6, "Sigaud - 2022 - Combining Evolution and Deep Reinforcement Learnin.pdf"}}
- [[Notes]]
	- # æ³¨é‡Š 
	   (2023/3/18 ä¸‹åˆ4:05:36)
	  
	  â€œA broader perspective and survey on all the evolutionary and rl combinations anterior to the advent of the so called Å¾deep learning" methods using large neural networks can be found in [19].â€ (Sigaud, 2022, p. 1) ä¹‹å‰çš„å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–ç®—æ³•çš„ç»“åˆæ–¹æ³•çš„ç»¼è¿°ï¼Œéœ€è¦è€ƒé‡ä¸€ä¸‹ã€‚
	  
	  â€œeven emerging libraries dedicated to their implementationâ€ (Sigaud, 2022, p. 1) è¿›åŒ–ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ ç»“åˆçš„åº“
	  
	  ä¸‰ç¯‡æ¯”è¾ƒè¿›åŒ–ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ çš„ç»¼è¿°
	  
	  â€œDeep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey.â€ (Sigaud, 2022, p. 18)
	  
	  â€œDerivative-free reinforcement learning: A review.â€ (Sigaud, 2022, p. 19)
	  
	  â€œPolicy Search in Continuous Action Domains: an Overviewâ€ (Sigaud, 2022, p. 20)
	  
	  â€œthe neural network architecture as a simple vector of parameters. This approach is known to require tedious hyperparameter tuning and generally perform worse than evolution strategies which are also mathematically more founded [9, 79]. In particular, the genetic operators used in erl and cerl based on a direct encoding have been shown to induce a risk of catastrophic forgetting of the behavior of eicient individuals.â€ (Sigaud, 2022, p. 4) ERLå’ŒCERLçš„ç¼ºç‚¹ï¼ŒåŸºäºè¿›åŒ–ç®—æ³•ï¼Œå‚æ•°ç¡¬ç¼–ç ï¼Œéœ€è¦è¶…å‚æ•°å·¨å¤§ï¼Œå¹¶ä¸”è¡¨ç°æ¯”è¿›åŒ–ç­–ç•¥å·®ï¼Œå­˜åœ¨é—å¿˜ä¼˜ç§€ä¸ªä½“çš„é£é™©
	  
	  â€œthe genetic operators of erl are replaced by operators using local replay bufersâ€ (Sigaud, 2022, p. 4)
	  
	  â€œNote that if an rl actor is injected in an evolutionary population and if evolution uses a direct encoding, the rl actor and evolution individuals need to share a common structure. Removing this constraint might be useful, as evolutionary methods are often applied to smaller policies than rl methods. For doing so, one might call upon any policy distillation mechanism that strives to obtain from a large policy a smaller policy with similar capabilities.â€ (Sigaud, 2022, p. 5) æŸ¥æ‰¾ç›¸å…³çš„è®ºæ–‡ï¼Œå­¦ä¹ ç­–ç•¥æç‚¼æœºåˆ¶ï¼Œå°†å¼ºåŒ–å­¦ä¹ æœºåˆ¶è½¬åŒ–æˆç±»ä¼¼èƒ½åŠ›çš„å°çš„ç­–ç•¥
	  
	  â€œA weakness of all the methods combining evolution and rl that we have listed so far is that they require evaluating the agents to perform the evolutionary selection step, which may impair sample eiciencyâ€ (Sigaud, 2022, p. 6) ç»“åˆè¿›åŒ–ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè¯„ä¼°æ™ºèƒ½ä½“æ¥æ‰§è¡Œé€‰æ‹©æ­¥éª¤ï¼Œå¯èƒ½æŸå®³æ ·æœ¬æ•ˆç‡
	  
	  è¿™ä¸ªç»¼è¿°ä¸æ˜¯ä¸€ä¸ªå¥½çš„ç»¼è¿°ï¼Œä»…ä»…æ˜¯å°†å„ä¸ªç®—æ³•è¿›è¡Œäº†ä¸€ä¸ªåˆ†ç±»ï¼Œå¹¶æ²¡æœ‰è®¤çœŸä»‹ç»æ¯ä¸ªç®—æ³•çš„å†…å®¹ã€‚
	  
	  Referred in [æ³¨é‡Š (2022/10/11 ä¸‹åˆ6:20:07)](zotero://note/u/JPH6GU2C/?ignore=1)
- ç»¼è¿°å°†è¿›åŒ–å¼ºåŒ–å­¦ä¹ åˆ†ä¸ºäº†å‡ ä¸ªä¸åŒçš„å†…å®¹ä¸»é¢˜ï¼š
	- 1. ä¸ºäº†æé«˜æ€§èƒ½ä½¿ç”¨è¿›åŒ–ç­–ç•¥ï¼Œè¿™ä¸€éƒ¨åˆ†æ˜¯æœ€ä¸»è¦çš„å†…å®¹ï¼Œæ¯”å¦‚ERLã€PDERLç­‰å†…å®¹