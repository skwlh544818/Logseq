tags:: [[å¾…è¯»]], [[æš‚æ—¶æ— ç”¨]]
date:: 2019
archive-location:: å·¥ç¨‹æŠ€æœ¯1åŒº Top
series-text:: æ— 
issue:: 6
series-title:: æ— 
doi:: 10.1109/TEVC.2019.2895748
title:: @Evolutionary generative adversarial networks
pages:: 921â€“934
volume:: 23
item-type:: [[journalArticle]]
call-number:: 16.497
rights:: 16.815
original-title:: Evolutionary generative adversarial networks
publication-title:: IEEE Transactions on Evolutionary Computation
archive:: Q1
journal-abbreviation:: Ieee T Evolut Comput
authors:: [[Chaoyue Wang]], [[Chang Xu]], [[Xin Yao]], [[Dacheng Tao]]
library-catalog:: è®¡ç®—æœºï¼šäººå·¥æ™ºèƒ½1åŒº  è®¡ç®—æœºï¼šç†è®ºæ–¹æ³•1åŒº
links:: [Local library](zotero://select/library/items/WT9XWB6B), [Web library](https://www.zotero.org/users/8746250/items/WT9XWB6B)

- [[Abstract]]
	- Generative adversarial networks (GANs) have been effective for learning generative models for real-world data. However, accompanied with the generative tasks becoming more and more challenging, existing GANs (GAN and its variants) tend to suffer from different training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary GANs (E-GANs) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a predefined adversarial objective function alternately training a generator and a discriminator, we evolve a population of generators to play the adversarial game with the discriminator. Different adversarial training objectives are employed as mutation operations and each individual (i.e., generator candidature) are updated based on these mutations. Then, we devise an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the well-performing offspring, contributing to progress in, and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.
- [[Attachments]]
	- [Snapshot](https://ieeexplore.ieee.org/abstract/document/8627945/) {{zotero-imported-file 99JEF7ZP, "8627945.html"}}
	- [Full Text](https://arxiv.org/pdf/1803.00657) {{zotero-imported-file J8LIEAD8, "Wang ç­‰ - 2019 - Evolutionary generative adversarial networks.pdf"}}
- tags:: [[/reading]], [[å¾…è¯»]], [[æš‚æ—¶æ— ç”¨]], [[ğŸ“’]]
  date:: 2019
  archive-location:: å·¥ç¨‹æŠ€æœ¯1åŒº Top
  series-text:: æ— 
  issue:: 6
  series-title:: æ— 
  doi:: 10.1109/TEVC.2019.2895748
  title:: @Evolutionary generative adversarial networks
  pages:: 921â€“934
  volume:: 23
  item-type:: [[journalArticle]]
  call-number:: 16.497
  rights:: 16.815
  original-title:: Evolutionary generative adversarial networks
  publication-title:: IEEE Transactions on Evolutionary Computation
  archive:: Q1
  journal-abbreviation:: Ieee T Evolut Comput
  authors:: [[Chaoyue Wang]], [[Chang Xu]], [[Xin Yao]], [[Dacheng Tao]]
  library-catalog:: è®¡ç®—æœºï¼šäººå·¥æ™ºèƒ½1åŒº  è®¡ç®—æœºï¼šç†è®ºæ–¹æ³•1åŒº
  links:: [Local library](zotero://select/library/items/WT9XWB6B), [Web library](https://www.zotero.org/users/8746250/items/WT9XWB6B)
- [[Abstract]]
	- Generative adversarial networks (GANs) have been effective for learning generative models for real-world data. However, accompanied with the generative tasks becoming more and more challenging, existing GANs (GAN and its variants) tend to suffer from different training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary GANs (E-GANs) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a predefined adversarial objective function alternately training a generator and a discriminator, we evolve a population of generators to play the adversarial game with the discriminator. Different adversarial training objectives are employed as mutation operations and each individual (i.e., generator candidature) are updated based on these mutations. Then, we devise an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the well-performing offspring, contributing to progress in, and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.
- [[Attachments]]
	- [Snapshot](https://ieeexplore.ieee.org/abstract/document/8627945/) {{zotero-imported-file 99JEF7ZP, "8627945.html"}}
	- [Full Text](https://arxiv.org/pdf/1803.00657) {{zotero-imported-file J8LIEAD8, "Wang ç­‰ - 2019 - Evolutionary generative adversarial networks.pdf"}}
- [[Notes]]
	- # æ³¨é‡Š 
	   (2023/4/9 ä¸‹åˆ3:31:53)
	  
	  â€œIf the data distribution and the generated distribution do not substantially overlap (usually at the beginning of training), the generator gradients can point to more or less random directions or even result in the vanishing gradient issue. GANs also suffer from mode collapse,â€ (Wang ç­‰, 2019, p. 1) GANç½‘ç»œä¼šå‡ºç°çš„é—®é¢˜
	  
	  â€œGANs also suffer from mode collapse,â€ (Wang ç­‰, 2019, p. 1)
	  
	  â€œFor example, although measuring Kullback-Leibler divergence largely eliminates the vanishing gradient issue, it easily results in mode collapse [24, 1]. Likewise, Wasserstein distance greatly improves training stability but can have non-convergent limit cycles near equilibrium [21].â€ (Wang ç­‰, 2019, p. 2) ä½¿ç”¨ä¸åŒçš„ç›®æ ‡å‡½æ•°æˆ–è€…è¯´è·ç¦»ä¼šå‡ºç°å„è‡ªçš„ä¼˜ç¼ºç‚¹