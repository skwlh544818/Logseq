- ## 回归问题
	- ### R平方决定系数
		- 因变量y的变化中有多少可以用自变量x来解释，是回归方程对观测值拟合程度的一种体现。比例越接近于1，表示当前的回归模型对数据的解释越好，越能精确描述数据的真实分布。
		- 在简单线性回归中，R平方相当于相关系数r的平方。
		- 计算公式：
		- $$R^2 = 1-\frac{SSR}{SST}=1-\frac{\sum_i^m(y-f)^2}{\sum_i^m(y-\hat y)^2}$$
		- SSR就是预测值与真实值之间的差异，SST是指真实值与真实值的均值之间的差异
		- **优点**：用于定量描述回归模型的解释能力。
		- **缺点**：没有考虑特征数量变化的影响。无法比较特征数目不同的回归模型。
	- ### 校正决定系数
		- 在R平方决定系数的基础上考虑特征个数的影响，可以消除样本数量和特征数量的影响
		- $$R^2\_adjusted = 1-\frac{(1-R^2)(m-1}{}$$
- ## 分类问题
	- precision 精确度：真正样本占预测正样本的比例
	- Recall 召回值：真正样本占总正样本的比例
	- ROC曲线：纵轴是真正率（真正样本比上总正样本），横轴是假正率（假正样本比上总负样本）
	- AUC：ROC曲线下面积，代表样本预测的排序质量，AUC越大，随即给定一个正样本和负样本，分类器将正样本的得分排在负样本前面的概率越大，越能够区分正负样本，性能越好
		- *从一个比较高的角度来认识AUC：仍然以异常用户的识别为例，高的AUC值意味着，模型在能够尽可能多地识别异常用户的情况下，仍然对正常用户有着一个较低的误判率（不会因为为了识别异常用户，而将大量的正常用户给误判为异常。*
- #机器学习 #分类 #回归
-
-